# python神经网络编程笔记

## 神经网络工作原理

（1）迭代算法：（在建立一个模型的基础上）得到一个答案，并多次改进，逼近预测值。

（2）预测器和分类器，用来训练这两者的真实实例叫做训练数据。

利用误差值改变参数。迭代算法对目标参数的调整应该是很微小的（避免最后一次改变与最后一次的训练数据过于匹配）

（3）调节系数（学习率），学习率的设定影响训练的最终效果。

（4）线性分类器的局限性

（e.g布尔逻辑函数，XOR函数，不能使用单个分类器解决分类问题）

（5）神经元：阈值概念（类似数学跃迁函数，平滑递增函数）

### 人工模型

神经元基本结构（叠加输入，进行输出，中间阈值判断），这是一个一个基本单位。

一大堆这样的单位以网状结构连接，构成人造模型。

网状结构原因：（1）容易编写计算机指令（2）在学习过程中会弱化其它不必要的连接。（模型对权重进行了调整，例如零权重就是断开了这条连接。）

### 矩阵

矩阵对计算机语言具有简洁性。可以简化计算。

矩阵元素不一定是数字，我们把它换成神经网络的元素，比如input_1,input_2,然后另一个矩阵中的元素是权重。

矩阵乘法的过程就是神经网络运算的过程。

```
X=W*I，W是权重矩阵，I是输入矩阵
```

x是组合调节后的信号，w是权重矩阵，i是输入矩阵

```
神经网络基础术语：
第一层是输入层，最后一层是输出层，中间是隐藏层。
```

每一层计算都是一次矩阵乘法，最后将输出值与预测值相对比，调节自己的权重。

### 多个节点的链接权重

每个链接权重对最终的预测值都有影响。

调节权重：

法一：平分误差。

法二：不等分，对较大的链接权重分配更多的误差。(例如百分比分配)

权重的两个用处：前向传播与反向传播。（输入层指向输出层为正向。）

### 反向传播误差

根据权重比例（单个/总体）分配自己的误差。

较远层的反向误差：每一层都按照相同思想分配误差，形象的比作反向传播，是因为误差的分配是沿着网络反向进行的。

~反向传播的起点是误差，因此用初始误差矩阵乘以权重矩阵。

简化：将w11/w11+w12简化为w11，这样仅仅失去反馈后误差的大小。

### 神经网络更新链接权重

基本方法：梯度下降法

设置多起点，尽量避免模型进行到错误的下降点。

误差测算：使用差的绝对值（有超调风险)，使用差的平方（越接近最小值，梯度越小）

三维空间的梯度下降：基本思想也是斜率，应用偏导数。

sigmoid函数是s型生长曲线，以此模拟神经元的阈值机制。

经过微分计算，误差函数的斜率如下：

```
k=-(t[k]-o[k])*sigmoid([求和]w[j,k]*o[j])(1-sigmoid[求和]w[j,k]*o[j])*oj
```

权重改变的方向与梯度变化方向相反。使用学习因子。（学习率）

### 准备输入数据

（1）输入：以s函数为例，如果输入变大，激活函数就会变得非常平坦。

权重的改变取决于激活函数的梯度，小梯度会限制学习能力，这是饱和式神经网络。

不能让输入信号太小，计算机会丧失精度。

（2）输出：

输出是最后一层节点弹出的信号，目标设置值一般都要与激活函数的值域相匹配。训练网络会驱使更大的权重，从而使网络饱和。





