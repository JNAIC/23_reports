# 王陈汉第四周（1）

## 第一部分：feature engineering

### creating features

（1）数学拟合   

首先我们用以下四个数据集起手：美国交通事故、1985 年汽车、混凝土配方和客户终身价值。

（p.s  kaggle上提出的特征创建小技巧，我总结一下就是信息采集和数据可视化）

```python
autos["stroke_ratio"] = autos.stroke / autos.bore

autos[["stroke", "bore", "stroke_ratio"]].head()
```

以上是数学变量的引入，利用比值定义出冲程比的概念，用于衡量数据。

另外的一些数学方法，指数对数的描述。

```python
# If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log
accidents["LogWindSpeed"] = accidents.WindSpeed.apply(np.log1p)

# Plot a comparison
fig, axs = plt.subplots(1, 2, figsize=(8, 4))
sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])
sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);
```

对数描述法，并且用数据可视化验证。

（2）计数

计数用于聚合特征，用sum的办法创建计数

```python
roadway_features = ["Amenity", "Bump", "Crossing", "GiveWay",
    "Junction", "NoExit", "Railway", "Roundabout", "Station", "Stop",
    "TrafficCalming", "TrafficSignal"]
accidents["RoadwayFeatures"] = accidents[roadway_features].sum(axis=1)

accidents[roadway_features + ["RoadwayFeatures"]].head(10)
```

前端列举了一些特征，并且这些特征的bool值通过sum函数累加。

下面展示了用表格内置功能进行数据整合的方法：

```python
components = [ "Cement", "BlastFurnaceSlag", "FlyAsh", "Water",
               "Superplasticizer", "CoarseAggregate", "FineAggregate"]
concrete["Components"] = concrete[components].gt(0).sum(axis=1)

concrete[components + ["Components"]].head(10)
```

(3)构建和分解功能

str 访问器允许您应用字符串方法，例如直接将 split 应用于列

（4）组转换
组转换可以聚合按某个类别分组的多行信息。 如果您发现了类别交互，可以用针对该类别的组转换。

使用聚合函数，组转换组合了两个特征：一个提供分组的分类特征和另一个要聚合其值的特征。“按州划分的平均收入”，可以选择“州”作为分组特征，选择“平均值”作为聚合函数，选择“收入”作为聚合特征。 为了在 Pandas 中计算这一点，使用 groupby 和 transform 

```python
ustomer["AverageIncome"] = (
    customer.groupby("State")  # for each state
    ["Income"]                 # select the income
    .transform("mean")         # and compute its mean
)

customer[["State", "Income", "AverageIncome"]].head(10)
```

Mean 函数，可以将它作为字符串传递来进行转换。 其他方便的方法包括 max、min、median、var、std 和 count，（python的基础数据处理方法）

在训练集上创建分组特征，导入验证集

```python
# Create splits
df_train = customer.sample(frac=0.5)
df_valid = customer.drop(df_train.index)

# Create the average claim amount by coverage type, on the training set
df_train["AverageClaim"] = df_train.groupby("Coverage")["ClaimAmount"].transform("mean")

# Merge the values into the validation set
df_valid = df_valid.merge(
    df_train[["Coverage", "AverageClaim"]].drop_duplicates(),
    on="Coverage",
    how="left",
)

df_valid[["Coverage", "AverageClaim"]].head(10)
```

（很多代码是照着kaggle打的）

### 使用k均值聚类

无监督算法——发现特征

聚类就是依照相似程度将数据分组。同时，我们为这些数据提供标签。

（应用于单个实值特征时，聚类的作用类似于传统的“分箱”或“离散化”变换。 在多个特征上，它就像“多维分箱”（有时称为矢量量化）。）

添加集群标签，把数据分成一个一个小块，更有利于模型的学习。

k均值聚类法是最简单的聚类算法，用普通直线距离衡量相似性，很多点是一个簇，k就是簇的数量。

（直白理解：簇在数据可视化中是一大堆密集点。）

p.s k均值聚类算法如何学习聚类

第一步：随便预设一些中心

第二步，将点分配给最近的中心，中心也在移动来最小化距离。

在这个过程中，我们唯一要决定的参量是k。

```python
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.cluster import KMeans

plt.style.use("seaborn-whitegrid")
plt.rc("figure", autolayout=True)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=14,
    titlepad=10,
)

df = pd.read_csv("../input/fe-course-data/housing.csv")
X = df.loc[:, ["MedInc", "Latitude", "Longitude"]]
```

在其中，挑出了两个特征（这种标准在过程中也可以变化。）

（我问了下gpt这段代码什么意思，我黏贴在这有助于我的理解：

1. `import` 语句导入了所需的Python库，包括matplotlib（用于绘图和可视化）、pandas（用于数据处理）、seaborn（用于美化图形）以及scikit-learn中的KMeans（用于K均值聚类算法）。
2. `plt.style.use("seaborn-whitegrid")` 设置了matplotlib绘图的样式，使用了"seaborn-whitegrid"样式，它使得图形有网格线，提高了可读性。
3. `plt.rc(...)` 一系列的`plt.rc()`语句设置了图表和轴的样式，包括字体、字号、粗细等参数。
4. `df = pd.read_csv("../input/fe-course-data/housing.csv")` 这一行代码用于从一个CSV文件中读取数据，并将数据存储在一个名为`df`的pandas数据帧（DataFrame）中。这个CSV文件的路径是`../input/fe-course-data/housing.csv`。
5. `X = df.loc[:, ["MedInc", "Latitude", "Longitude"]]` 这行代码从数据帧`df`中选择了三列数据，分别是"MedInc"（收入中位数）、"Latitude"（纬度）、"Longitude"（经度），并将它们存储在一个名为`X`的新数据帧中。
6. `X.head()` 打印了`X`数据帧的前几行数据，以便查看数据的样本。）

### 主成分分析（PCA）

pca是数据变化的一种。

kaggle举了一个例子，以一个主要特征为一个数据轴，，当然，我的理解就是可以有多条轴，比如说我有三个特征，就可以构造一个xyz三维的空间。

PCA 的全部思想：我们不是用原始特征来描述数据，而是用它的变化轴来描述它。 变化的轴成为新的特征。

你pca结束的那个图像描述，实际上只是一堆的特征的加权和。

这些特征同时叫做主成分。

```python
df["Size"] = 0.707 * X["Height"] + 0.707 * X["Diameter"]
df["Shape"] = 0.707 * X["Height"] - 0.707 * X["Diameter"]
```

这边就是描述了两个变化轴。分别是大小和形状。

0.707就是权重，权重也就是载荷。

0.707和-0.707表示这个载荷是向两个不同方向变化的，但是绝对值相同，表示载荷相同。

比如说我描述时用了几个特征，这几个变量会变化，pca通过每个变量的解释方差来描述这一点。

这个方差决定于你要预测的内容。

2.怎么用pca做特征工程

（1）pca作为一种描述的技术，比如说我描述大小，就是物体长度的乘积作为一个主成分。

（2）pca是一个组件。比如说：

降维：对于方差约为0的这种变量，我直接删除它。

异常检测：在低方差组件中，异常成分通常会产生大量信息便于我们检测。

降噪：将信息丰富的信号储存到较少数量的特征中，同时保留噪声，提高信息部分的占比。

去相关：一些机器学习算法难以应对高度相关的特征。 PCA 将相关特征转换为不相关成分，这可以让您的算法更容易使用。（不懂）

```python
def plot_variance(pca, width=8, dpi=100):
    # Create figure
    fig, axs = plt.subplots(1, 2)
    n = pca.n_components_
    grid = np.arange(1, n + 1)
    # Explained variance
    evr = pca.explained_variance_ratio_
    axs[0].bar(grid, evr)
    axs[0].set(
        xlabel="Component", title="% Explained Variance", ylim=(0.0, 1.0)
    )
    # Cumulative Variance
    cv = np.cumsum(evr)
    axs[1].plot(np.r_[0, grid], np.r_[0, cv], "o-")
    axs[1].set(
        xlabel="Component", title="% Cumulative Variance", ylim=(0.0, 1.0)
    )
    # Set up figure
    fig.set(figwidth=8, dpi=100)
    return axs
```

我拉取了一个函数，这个函数用于描述pca的方差解释率图表

问gpt对这个函数的解释：

1. `def plot_variance(pca, width=8, dpi=100):`：这是一个自定义的函数 `plot_variance`，它接受三个参数，其中 `pca` 是一个已经进行PCA降维的对象，`width` 和 `dpi` 是用于设置图形宽度和分辨率的可选参数。

2. `fig, axs = plt.subplots(1, 2)`：这一行代码创建了一个包含两个子图的图形对象。`plt.subplots(1, 2)` 创建了一个一行两列的子图布局，返回 `fig`（整个图形对象）和 `axs`（包含两个子图的数组）。

3. `n = pca.n_components_`：这里获取了PCA中的主成分数量，通常表示为 `n_components_`。

4. `grid = np.arange(1, n + 1)`：创建一个包含从1到主成分数量 `n` 的整数的数组，用于表示主成分的序号。

5. `evr = pca.explained_variance_ratio_`：获取PCA模型的每个主成分的解释方差比率，存储在 `evr` 中。

6. `axs[0].bar(grid, evr)`：在第一个子图上创建柱状图，x轴表示主成分的序号，y轴表示解释方差比率。

7. `axs[0].set(xlabel="Component", title="% Explained Variance", ylim=(0.0, 1.0))`：设置第一个子图的标签、标题和y轴的限制，使其在0到1之间。

8. `cv = np.cumsum(evr)`：计算解释方差比率的累积和，将结果存储在 `cv` 中。

9. `axs[1].plot(np.r_[0, grid], np.r_[0, cv], "o-")`：在第二个子图上创建一条线图，x轴表示主成分的序号（包括0作为起点），y轴表示累积解释方差。

10. `axs[1].set(xlabel="Component", title="% Cumulative Variance", ylim=(0.0, 1.0))`：设置第二个子图的标签、标题和y轴的限制，也使其在0到1之间。

11. `fig.set(figwidth=8, dpi=100)`：设置整个图形的宽度和分辨率。

12. 最后，函数返回 `axs`，其中包含了两个子图对象。

    下面，是一个计算互信息的函数:

    ```python
    def make_mi_scores(X, y, discrete_features):
        mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
        mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
        mi_scores = mi_scores.sort_values(ascending=False)
        return mi_scores
    ```

    1. `def make_mi_scores(X, y, discrete_features):`：这是一个自定义的函数，它接受三个参数：

       - `X`：包含特征数据的DataFrame或数组。
       - `y`：包含目标变量数据的Series或数组。
       - `discrete_features`：一个布尔数组或列表，用于指示哪些特征是离散型的（True表示离散，False表示连续）。

    2. `mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)`：这一行代码使用`mutual_info_regression`函数来计算特征和目标变量之间的互信息得分。`mutual_info_regression`是一个用于回归问题的互信息计算函数，它通过`X`（特征数据）、`y`（目标变量数据）和`discrete_features`（离散特征指示）来计算互信息得分。得到的`mi_scores`是一个包含特征名称和对应互信息得分的数组。

    3. `mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)`：将互信息得分转换为Pandas Series对象，并为该Series命名为"MI Scores"，同时使用特征的列名作为索引。

    4. `mi_scores = mi_scores.sort_values(ascending=False)`：对互信息得分进行降序排序，以便最重要的特征排在前面。

    5. `return mi_scores`：最后，函数返回排序后的互信息得分Series，该Series包含了每个特征与目标变量之间的互信息得分，按重要性降序排列。

       (补充数据标准化：数据标准化的目的是消除不同特征之间的尺度差异，确保它们具有相似的权重，以避免某些特征对模型的影响过大。)

数据标准化函数：
```python
features = ["highway_mpg", "engine_size", "horsepower", "curb_weight"]

X = df.copy()
y = X.pop('price')
X = X.loc[:, features]

# Standardize
X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)
```

仅解释最后一行：数据减去自身均值再除以标准差，这是一种数据标准化的方法。

```python
from sklearn.decomposition import PCA

# Create principal components
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Convert to dataframe
component_names = [f"PC{i+1}" for i in range(X_pca.shape[1])]
X_pca = pd.DataFrame(X_pca, columns=component_names)

X_pca.head()
```

这段代码涉及到主成分分析（PCA）的应用，用于将原始数据进行降维和变换。让我逐步解释这些代码的主要部分：

1. `from sklearn.decomposition import PCA`：导入了Scikit-Learn库中的PCA模块，以便使用PCA进行降维。
2. `pca = PCA()`：创建了一个PCA对象，没有传入任何参数。这意味着PCA将使用默认设置，即将数据降维到包含所有主成分的维度。
3. `X_pca = pca.fit_transform(X_scaled)`：这一行代码将标准化后的数据 `X_scaled` 输入到PCA模型中，并使用 `fit_transform` 方法进行降维。`fit_transform` 方法将数据投影到新的主成分空间，并返回降维后的数据。`X_pca` 现在包含了原始数据在主成分空间中的表示。
4. `component_names = [f"PC{i+1}" for i in range(X_pca.shape[1])]`：这一行代码创建了一个包含主成分名称的列表。默认情况下，PCA将主成分命名为 "PC1"，"PC2"，"PC3" 等，其中 "PC1" 是最重要的主成分，"PC2" 是第二重要的主成分，依此类推。这里使用列表推导式创建了这些名称。
5. `X_pca = pd.DataFrame(X_pca, columns=component_names)`：将降维后的数据 `X_pca` 转换为一个Pandas数据框，其中每一列被命名为 "PC1"、"PC2"、"PC3" 等，以对应于主成分的名称。这样可以更容易地分析和可视化降维后的数据。
6. `X_pca.head()`：显示了数据框 `X_pca` 的前几行，以便查看降维后的数据。

### 目标编码

目标编码适用于分类特征。 它是一种将类别编码为数字的方法，同之处在于它还使用目标来创建编码。 这是监督特征工程技术。

要点：数字替换特征类别。

```python
autos["make_encoded"] = autos.groupby("make")["price"].transform("mean")

autos[["make", "price", "make_encoded"]].head(10)
```

这段代码表示了用平均值作为目标编码。

问题：未知类别导致过度拟合，必须要挑出应有的缺失值。//样本太小，计算平均值可能样本本来就不多。

所以目标编码适用于高基数和领域驱动。

（分割：划分子集，适应任务。）

```python
from category_encoders import MEstimateEncoder

# Create the encoder instance. Choose m to control noise.
encoder = MEstimateEncoder(cols=["Zipcode"], m=5.0)

# Fit the encoder on the encoding split.
encoder.fit(X_encode, y_encode)

# Encode the Zipcode column to create the final training data
X_train = encoder.transform(X_pretrain)
```

1. `from category_encoders import MEstimateEncoder`：首先，导入了 `category_encoders` 库中的 `MEstimateEncoder` 类，这是一个用于进行目标编码的工具。
2. `encoder = MEstimateEncoder(cols=["Zipcode"], m=5.0)`：创建了一个 `MEstimateEncoder` 的实例 `encoder`，并指定了要进行编码的列为 "Zipcode"。此外，还指定了参数 `m` 为5.0，它控制了噪音的程度，较大的 `m` 值会减小编码的影响，较小的 `m` 值会增加编码的影响。
3. `encoder.fit(X_encode, y_encode)`：使用编码集 `X_encode` 和对应的目标值 `y_encode` 来拟合（训练）目标编码器 `encoder`。这一步的目的是计算并学习 "Zipcode" 列中每个类别的目标编码映射。
4. `X_train = encoder.transform(X_pretrain)`：将训练集 `X_pretrain` 中的 "Zipcode" 列使用已经拟合好的目标编码器 `encoder` 进行编码，生成新的特征列 "Zipcode"，并将其赋值给 `X_train`。这个编码的结果将用于训练机器学习模型。

综合起来，这段代码的主要目的是使用目标编码器 `MEstimateEncoder` 对训练数据中的 "Zipcode" 列进行编码，以将其转换为数值特征。这种编码方法会将每个 "Zipcode" 类别映射到与该类别相关的目标变量（通常是均值）的数值。这有助于模型使用分类变量的信息，同时将其转化为可供机器学习模型使用的数值特征，从而提高模型性能。

