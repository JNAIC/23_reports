# 王陈汉第三周（1）

主要任务：（1）kaggle收尾及特征工程

（2）神经网络入门

## 损失函数

损失函数就是衡量估测数据与实际值的函数，一切训练目的是将损失函数最小化。

简单了解三种损失函数：

（1）均方误差损失函数，说白了就是回归分析，用平方差算离散程度。

（2）交叉熵损失函数用于分类，衡量模型输出的概率分布与实际标签之间的差异。

（3）Hinge损失函数用于二元分类。

要点：根据解决问题类型选择损失函数，优化算法减小损失函数。

### 代码实操

（1）MSE的简单例子

```python
import numpy as np

# 定义均方误差损失函数
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# 生成示例数据
y_true = np.array([1, 2, 3, 4, 5])  # 真实值
y_pred = np.array([0.8, 2.5, 2.8, 3.6, 4.2])  # 预测值

# 计算均方误差
mse = mean_squared_error(y_true, y_pred)
print("均方误差为:", mse)
```

个人理解，该例子中，通过mse计算了五个预测值和实际值的平方差，并通过求均值来计算出一个平均的误差水平。

要注意，如pytorch等框架都具有现成函数提供运算。

（2）正则化的概念

机器学习中经常会在损失函数中加入正则项，称之为正则化。

**目的**：防止模型过拟合

**原理**：在损失函数上加上某些规则（限制），缩小解空间，从而减少求出过拟合解的可能性

通俗理解：给损失函数加入限制，让他们不至于过拟合。

比如我们预测一个函数模型，当它无法表示绝大部分的数据时，我们称之为泛化能力过低；当它几乎可以表示每一个值，则会导致很多不必要的噪声点也被加入了进去，因此此时需要正则化来解决问题。

正则化底层逻辑：

使用回归模型举例子：正是那些高次项导致了过拟合的产生，所以如果我们能让这些高次项的系数接近于0的话，就可以提高精度，高次项会导致过拟合。我们在方程中增加了两个限制条件，分别对  和  进行限制，不能让他们过高。很直观的看出，要想使  最小化，不仅仅需要  足够拟合 ，同时还需要减少  和 。

一方面要使得L(w)的取值最小，必然w的绝对值会取到很大，这样模型才能完美拟合训练样本点；另一方面，当w的绝对值很大时，||w||的值又会变得很大，因此为了权衡，只有使得w取值适当，才能保证值取到最优。这样得到的拟合曲线平滑很多，因此具有泛化能力。这就是正则化存在的意义，能帮助我们在训练模型的过程中，防止模型过拟合。

### 梯度下降简单底层逻辑

梯度下降法在机器学习中常常用来优化损失函数，是一个非常重要的工具。正弦函数中有多个极值点。梯度下降法的作用就是寻找一个 **「极小值点」** （在本篇文章讨论极小值，顾名思义：梯度下降法），从而让函数的值尽可能地小。相信你也发现了，这么多个极值点，那么梯度下降法找到的是哪一个点呢？关于这个问题就要看运气了，算法的最开始会 **「随机」** 寻找一个位置然后开始搜索**「局部」**的最优解。

同理，偏导数用于解决三维空间的梯度下降。

代码实现：

```python
import numpy as np

# 模拟数据集
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 初始化参数
theta = np.random.randn(2,1)

# 设置迭代次数和学习率
iterations = 1000
learning_rate = 0.1
m = len(X)

# 梯度下降迭代
for iteration in range(iterations):
    gradients = 2/m * X.T.dot(X.dot(theta) - y)
    theta = theta - learning_rate * gradients

# 最终得到的参数
print(theta)
```

### 学习率的概念

相当于我用多大的精度去进行学习，比如说对一个二次函数，我可以1为单位进行条约梯度下降，也可以更加精细，当然，过于精细会影响学习速度，太过粗略会导致模型预测结果在预期周围震荡。

最好的就是，先设置大的学习率，然后逐渐减小。对于**深度学习**来说，每 t 轮学习，学习率减半。

## 传播

**正向传播（forward-propagation）**：指对[神经网络](https://so.csdn.net/so/search?q=神经网络&spm=1001.2101.3001.7020)沿着输入层到输出层的顺序，依次计算并存储模型的**中间变量**。

**反向传播（back-propagation）**：沿着从输出层到输入层的顺序，依据链式法则，依次计算并存储目标函数有关神经网络各层的**中间变量**以及**参数的梯度**。

一方面，**正向传播的计算可能依赖于\**模型参数的当前值\****，而这些模型参数是在反向传播梯度计算后通过优化算法迭代的。

另一方面，**反向传播的梯度计算可能依赖于\**各变量的当前值\****，而这些变量的当前值是通过正向传播计算得到的。

因此，在参数初始化完成后，我们交替进行正向传播和方向传播。
