#陈丽媛第六周周报

#感觉这周就准备期中考试了，其他没干多少

#查了点线性回归的资料

线性回归用于预测输入变量和输出变量之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生的变化。
回归模型正是表示从输入变量到输出变量之间映射的函数，回归问题的学习等价于函数拟合。 

#梯度下降

三种方法
1.批量梯度下降法:批量梯度下降法每次都使用训练集中的所有样本更新参数。
它得到的是一 个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么迭代速度就会变得很慢。
优点：可以得出全局最优解。
缺点：样本数据集大时，训练速度慢
2.随机梯度下降法:随机梯度下降法每次更新都从样本随机选择1组数据，因此随机梯度下降比批量梯度下降在计算量上会大大减少。
SGD有一个缺点是，其噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。
而且SGD因为每次都是使用一个样本进行迭代，因此最终求得的最优解往往不是全局最优解，而只是局部最优解。
但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近。 
优点：训练速度较快。
缺点：过程杂乱，准确度下降。
3.小批量梯度下降法:小批量梯度下降法对包含n个样本的数据集进行计算。
综合了上述两种方法，既保证了训练速度快，又保证了准确度。    

#步骤:
假设函数只有一个极小点。
初始给定参数。
方法：①首先设定一个较小的正数 α , ε;②求当前位置出处的各个偏导数③修改当前函数的参数值，④如果参数变化量小于 ，退出；否则返回第2步。
