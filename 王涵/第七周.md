第七周

这周主要是在期中考试，python学习什么的又是比较少/_ \

**python学习**

1）力扣上的每日一练，坚持了一半吧

2）温习了一些numpy

**作业**

代码如下

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt

	# 读取数据
	shuju = pd.read_csv("C:\\Users\\15200\\Documents\\周报\\人口利润\\test_task\\ex1data1.txt", names=['renkou', 'lirun'], header=None)

	# 下面定义一个梯度下降算法
    m = len(shuju)
	x0 = np.ones((m, 1))
	x1 = shuju.iloc[:, 0].values.reshape(m, 1)
	X = np.hstack((x0, x1))
	Y = shuju.iloc[:, 1].values.reshape(m, 1)

	xuexilv = 0.01


    def gradient_function(theta, X, Y):
    # 这是梯度函数，用于计算梯度
    diff = np.dot(X, theta) - Y
    # 我们通过np.dot(X, theta)可以得到估算值，并减去Y取得残差
    return (1 / m) * np.dot(X.transpose(), diff)
    # 这里返回值用到了微分后的代价函数


    def gradient_descent(X, Y, alpha):
    # 结合上面的代码，我们接着定义一个梯度下降函数
    theta = np.array([1, 1]).reshape(2, 1)
    # 我们创建一个数组theta，用来迭代参数
    gradient = gradient_function(theta, X, Y)
    # 这里计算梯度
    while not all(abs(gradient) <= 1e-5):
        # 这里开始进行梯度下降，当梯度小于1e-5时，停止迭代
        theta = theta - alpha * gradient
        # 这里进行梯度下降，alpha即为学习率
        gradient = gradient_function(theta, X, Y)
    return theta


	optimal = gradient_descent(X, Y, xuexilv)

	# 根据数据画出对应的图像
    def plot(X, Y, theta):

    ax = plt.subplot(111)
    ax.scatter(X[:, 1], Y, s=5, c="blue", marker="o")
    # 这里是在画出散点图

    plt.xlabel("population")
    plt.ylabel("profit")
    x = np.arange(0, 21, 10)
    # 这里大可以把步长取大一点
    y = theta[0] + theta[1] * x
    ax.plot(x, y)
    plt.show()


	plot(X, Y, optimal)
