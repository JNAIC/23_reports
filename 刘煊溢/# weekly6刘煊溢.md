### 呃呃呃，又见面了

### 感觉这周忙着弄例会，还有迎新晚会的排练时间并不充裕呢

# 补充一下例会上的东西

### 首先，神经网络的层级划分并不包括输入层，那为什么一定要至少两层呢？

### 因为只有一层的神经网络只能处理线性可分问题，而现实生活当中往往问题都是非线性的

### 多层的神经网络引入隐藏层可以学习到更为复杂的特征

### 这就有点深度学习的味道了

### 还有我想再解释一下sigmoid激活函数其实是将数据进行了一个压缩，把值都压缩到0和1之间

### 那有什么用呢？

### 我们就可以抽象的将sigmoid函数看作是一个“分类概率”，达到一个分类的目的

### 还有就是sigmoid函数其实也有一个很大的缺点就是容易造成梯度缺失，因为当值很大（或很小）的时候它总是趋近于

### 1（或0），即函数饱和

### 这怎么办呢？

### 于是我们找到了一个可以解决这一问题的另一个函数Relu，当x<0时，输出为0，当x>0时，输出结果为x。

### 因为它不会饱和，至少在x>0时是这样的

### 还有就是在每一层的输入信息时，我们通常还会在加一个b，这个b可以理解为为了达到更好的目标而做出的一个调整

### 它的专业名称为偏置

### 还有一个问题就是神经元之间的链接问题，其实在神经网络当中只有在全连接神经网络中每层的神经元之间相互链接

### 而在其他一些神经网络当中可以不是全部链接

### 还有就是卷积，卷积有点类似于矩阵中的分块，在图像处理方面应用广泛

### 他的工作原理就是将图片分区，在图像识别中利用未知图片的局部和已知正确的图片局部进行对比的计算过程

### 提到卷积就不得不提到滤波器了

### 什么是滤波器呢？

### 它其实和滤波器很相似，就是通过一个固定的权数矩阵来处理，多个滤波器叠加就构成了卷积层

### 感觉这个还是有点抽象，那个佬来讲讲（坏笑）

# 再说说线性回归作业的事情

### 由于忙别的事情，这个还没怎么动工

### 不过还是复习了一下pandas的用法，在anaconda上导入了数据，之后的还在进行当中……

### 嘤嘤嘤，下周还有期中考试，还得复习……
