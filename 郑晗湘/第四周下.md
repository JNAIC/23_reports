---
attachments: [Clipboard_2024-03-18-10-46-21.png, Clipboard_2024-03-18-10-53-18.png, Clipboard_2024-03-18-11-28-39.png]
title: 大致原理了解：（还没来得及全部代码展示）
created: '2024-03-17T15:42:24.193Z'
modified: '2024-03-18T03:28:39.728Z'
---

大致原理了解：（还没来得及全部代码展示）

感知机是人工神经网络
在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。感知机的权值是由训练得到的

首先是单层感知机即单层神经网络

同样分为输入输出和隐藏层，但是在“感知机”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。

我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们根据计算层的数量来命名。

　　假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWFnZXMyMDE1LmNuYmxvZ3MuY29tL2Jsb2cvNjczNzkzLzIwMTUxMi82NzM3OTMtMjAxNTEyMzAyMDU0Mzc5OTUtNjczODU2NjQ0LmpwZw?x-oss-process=image/format,png)

其中计算可以看做矩阵的乘法，使计算简洁

而后是多层感知机即两层神经网络
添加了中间层，权值也随之增加。与单层相同它也可以由矩阵表示，或者增加输出单元
![](@attachment/Clipboard_2024-03-18-10-46-21.png)
与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。它可以做非线性分类，因为隐藏层对原始的数据进行了一个空间变换使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。
相对的就是我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。从而方便进行非线性分类。
![](@attachment/Clipboard_2024-03-18-10-53-18.png)

*困扰我许久的损失函数* ：*必须要得到一个目标值使所有参数对其损失即相差值变小，这就涉及到参数优化，所以采用梯度下降（两个变量的优化方法），使参数与目标值的损失函数达到最低，最接近真实模型。*
深度学习即多层神经网络
继续添加中间层。增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。
![](@attachment/Clipboard_2024-03-18-11-28-39.png)



