
# 开完例会 有的没有完全理解  这周主要是理论学习阶段 下周进行代码的实现


· 回归 ———>> 连续变量
· 分类 ———>> 不连续变量  离散

## 线性回归

m：训练集中实例的数量
n：特征的个数 （单变量中 n=1 ）
x：表示特征/输入变量
y：表示目标变量/输出
（x，y）：训练集中实例

数据集是一个二维数组

假设函数  决定 ——>模型
损失函数  决定 ——>策略

#### 模型 ：h  theta （x）= theta 0  + theta 1 x
其中 x 是给了的数据集  theta是未知的 需要训练的

目标：找到最合适的 theta 0 和 theta 1

#### 损失函数：J（theta 0 ，theta 1）= 1/2m ∑ （h theta（x）- y ）²

![[Pasted image 20231110223133.png]]

此处损失函数用的是平方和计算 而不用普通的差值 也不用绝对值损失 前者容易正负抵消 后者 0 处不可导

损失函数中我们就要找到抛物面的最低点 （梯度下降）也就是下山的最快速度 即负梯度  求函数极值


## 梯度下降算法 （Gradient Descent）
    ——是一个用来求函数最小值的算法

其背后的思想：

1.  随机选择一个参数的组合  计算代价函数
2. 然后我们寻找下一个能让代价函数数值最低的参数组合
3. 持续这么做 直到一个局部最小值 （local minimum）


##### 如何理解梯度？

在单变量的函数中，梯度就是函数的微分 代表着函数在某个给定点的切线斜率 
在多变量函数中，梯度是函数对每个变量的偏微分组成的向量  他是函数给定点的下降最快的方向



##### 参数更新方程

![[Pasted image 20231110223237.png]]
![[Pasted image 20231110221253.png]]

对theta赋值 是 J（theta）按照梯度下降的方向进行 一直迭代下去 最终得到局部最小值
α是学习率 他决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大
步子会越来越小 因为斜率不断减小
